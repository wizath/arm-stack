#!/usr/bin/env python3
"""
ARM Stack Usage Analyzer

A comprehensive tool for analyzing stack usage in ARM embedded projects.
This tool parses object files and stack usage information to provide detailed
analysis of function call graphs and stack consumption patterns.

Originally inspired by avstack.pl, this implementation provides enhanced
visualization, filtering capabilities, and better error handling.

Requirements:
    - ARM GCC toolchain (arm-none-eabi-objdump)
    - Project compiled with -fstack-usage flag
    - Object files (.o or .c.obj) and stack usage files (.su)

Author: wizath
GitHub: https://github.com/wizath
License: MIT
Version: 1.2
"""

import sys
import os
import re
import subprocess
import glob
import json
import logging
from collections import defaultdict, namedtuple
from typing import Dict, Set, List, Optional
import argparse
from pathlib import Path
from datetime import datetime

# Version information
__version__ = "1.2"
__author__ = "wizath"
__github__ = "https://github.com/wizath"


class Colors:
    """ANSI color codes for enhanced terminal output"""
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'


# Tool configuration constants
DEFAULT_OBJDUMP = "arm-none-eabi-objdump"
CALL_COST = 4  # ARM function call overhead in bytes

# Data structure for function information
FunctionInfo = namedtuple('FunctionInfo', [
    'name',              # Function name (cleaned)
    'cost',              # Total stack cost including callees
    'frame_size',        # Local frame size from .su file
    'height',            # Call chain depth
    'is_recursive',      # Whether function is recursive
    'is_root',           # Whether function is a root (entry point)
    'is_unbounded',      # Whether function has unbounded stack usage
    'unbounded_reason'   # Reason for unbounded stack (recursion, function_pointer, etc.)
])


class StackAnalyzer:
    """
    Main analyzer class for processing ARM object files and calculating stack usage.

    This class implements the core algorithm for:
    - Parsing object file disassembly to build call graphs
    - Reading stack usage information from .su files
    - Resolving function symbols across object files
    - Calculating total stack costs through call graph traversal
    - Generating comprehensive analysis reports
    """

    def __init__(
            self,
            build_dir: str,
            threshold: int = 10,
            show_warnings: bool = True,
            objdump_path: str = DEFAULT_OBJDUMP):
        """
        Initialize the stack analyzer.

        Args:
            build_dir: Path to build directory containing object files
            threshold: Minimum stack usage threshold for reporting (bytes)
            show_warnings: Whether to display symbol resolution warnings
            objdump_path: Path to objdump executable
        """
        self.build_dir = Path(build_dir)
        self.threshold = threshold
        self.show_warnings = show_warnings
        self.objdump_path = objdump_path
        self.logger = logging.getLogger(__name__)

        # Core data structures for analysis
        self.frame_size: Dict[str, int] = {}  # "func@file" -> frame size in bytes
        self.call_graph: Dict[str, Set[str]] = defaultdict(set)  # "func@file" -> {callees}
        self.total_cost: Dict[str, int] = {}  # "func@file" -> total stack cost
        self.call_depth: Dict[str, int] = {}  # "func@file" -> maximum call depth
        self.visited: Dict[str, str] = {}  # "func@file" -> traversal status
        self.has_caller: Set[str] = set()  # Functions called by others
        self.unresolved: Set[str] = set()  # Unresolved function references

        # Symbol resolution tables
        self.addresses: Dict[int, str] = {}  # address -> "func@file"
        self.global_name: Dict[str, str] = {}  # func_name -> "func@file"
        self.ambiguous: Set[str] = set()  # Functions with ambiguous names

        # Function pointer detection
        self.has_function_pointer: Set[str] = set()  # Functions with function pointer calls
        self.rtl_available: bool = False  # Whether RTL dumps are available

        # Git repository information
        self.git_info: Dict[str, str] = {}  # Git hash, branch, etc.

    def find_object_files(self) -> List[str]:
        """
        Locate all relevant object files in the build directory.

        Filters out CMake compiler identification files and other non-project objects.

        Returns:
            List of object file paths

        Raises:
            FileNotFoundError: If build directory doesn't exist or no object files found
        """
        if not self.build_dir.exists():
            raise FileNotFoundError(f"Build directory '{self.build_dir}' does not exist")

        obj_files = []
        # Support both .o and .c.obj extensions (Zephyr/CMake uses .c.obj)
        for pattern in ["**/*.o", "**/*.c.obj"]:
            all_files = glob.glob(str(self.build_dir / pattern), recursive=True)
            # Filter out CMake-generated compiler test files
            for file in all_files:
                if not any(exclude in file for exclude in [
                    'CompilerIdC', 'CompilerIdCXX', 'CMakeCCompilerId', 'CMakeCXXCompilerId'
                ]):
                    obj_files.append(file)

        if not obj_files:
            raise FileNotFoundError(f"No relevant object files (.o or .c.obj) found in '{self.build_dir}'")

        return sorted(obj_files)

    def _find_rtl_files(self, obj_files: List[str]) -> List[str]:
        """
        Find RTL dump files corresponding to object files.

        RTL files are generated with -fdump-rtl-dfinish and have extensions like:
        - .c.270r.dfinish (GCC 5.x)
        - .c.271r.dfinish (GCC 6.x)
        - .c.272r.dfinish (GCC 7.x+)

        Args:
            obj_files: List of object file paths

        Returns:
            List of RTL dump file paths
        """
        rtl_files = []

        for obj_file in obj_files:
            # Get base name for RTL file search
            if obj_file.endswith('.o'):
                base_name = obj_file[:-2]  # Remove .o
            elif obj_file.endswith('.c.obj'):
                base_name = obj_file[:-6]  # Remove .c.obj
            else:
                continue

            # Search for RTL dump files with various extensions
            rtl_pattern = f"{base_name}.c.*r.dfinish"
            matching_files = glob.glob(rtl_pattern)

            if matching_files:
                # Use the first matching RTL file (should be only one)
                rtl_files.append(matching_files[0])

        return rtl_files

    def _detect_git_info(self) -> None:
        """
        Detect git repository information from current working directory.

        Extracts:
        - Git commit hash (full and short)
        - Current branch name
        - Repository status (clean/dirty)
        - Remote origin URL (if available)
        """
        try:
            # Check if we're in a git repository
            result = subprocess.run(['git', 'rev-parse', '--is-inside-work-tree'],
                                    capture_output=True, text=True, cwd=os.getcwd())

            if result.returncode != 0:
                self.logger.debug("Not in a git repository")
                return

            # Get commit hash (full)
            result = subprocess.run(['git', 'rev-parse', 'HEAD'],
                                    capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                self.git_info['commit_hash'] = result.stdout.strip()
                self.git_info['commit_hash_short'] = result.stdout.strip()[:8]

            # Get current branch name
            result = subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
                                    capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                self.git_info['branch'] = result.stdout.strip()

            # Check if repository is clean
            result = subprocess.run(['git', 'status', '--porcelain'],
                                    capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                self.git_info['is_clean'] = len(result.stdout.strip()) == 0
                self.git_info['status'] = 'clean' if self.git_info['is_clean'] else 'dirty'

            # Get remote origin URL (if available)
            result = subprocess.run(['git', 'remote', 'get-url', 'origin'],
                                    capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                self.git_info['remote_url'] = result.stdout.strip()

            # Get commit message (first line)
            result = subprocess.run(['git', 'log', '-1', '--pretty=format:%s'],
                                    capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                self.git_info['commit_message'] = result.stdout.strip()

            # Get commit author and date
            result = subprocess.run(['git', 'log', '-1', '--pretty=format:%an'],
                                    capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                self.git_info['commit_author'] = result.stdout.strip()

            result = subprocess.run(['git', 'log', '-1', '--pretty=format:%ci'],
                                    capture_output=True, text=True, cwd=os.getcwd())
            if result.returncode == 0:
                self.git_info['commit_date'] = result.stdout.strip()

            if self.git_info:
                branch = self.git_info.get('branch', 'unknown')
                commit = self.git_info.get('commit_hash_short', 'unknown')
                self.logger.info(f"  ‚úì Git repository detected: {branch}@{commit}")
                if not self.git_info.get('is_clean', True):
                    self.logger.info("  ‚ö† Repository has uncommitted changes")

        except FileNotFoundError:
            self.logger.debug("Git not available")
        except Exception as e:
            if self.show_warnings:
                self.logger.warning(f"‚ö†Ô∏è Error detecting git info: {e}")

    def check_prerequisites(self) -> None:
        """
        Verify that all required tools and files are available for analysis.

        Checks for:
        - ARM objdump tool availability
        - Build directory existence
        - Presence of object files and stack usage files

        Returns:
            List of object files to process

        Raises:
            SystemExit: If prerequisites are not met
        """
        self.logger.info("üîç Checking prerequisites...")

        # Detect git repository information
        self._detect_git_info()

        # Verify ARM toolchain availability
        try:
            subprocess.run([self.objdump_path, "--version"], capture_output=True, check=True)
            self.logger.info(f"  ‚úì {self.objdump_path} found")
        except (subprocess.CalledProcessError, FileNotFoundError):
            error_msg = (f"{Colors.RED}‚ùå Error: {self.objdump_path} not found. "
                         f"Please install ARM GCC toolchain or specify correct path with --objdump.{Colors.END}")
            print(error_msg)
            sys.exit(1)

        # Verify build directory
        if not self.build_dir.exists():
            print(f"{Colors.RED}‚ùå Error: Build directory '{self.build_dir}' does not exist.{Colors.END}")
            print(f"{Colors.YELLOW}üí° Please run cmake and build the project first.{Colors.END}")
            sys.exit(1)

        # Locate object files
        obj_files = self.find_object_files()
        self.logger.info(f"  ‚úì Found {len(obj_files)} object files")

        # Verify stack usage files exist
        su_files = []
        for obj_file in obj_files:
            # Handle both .o and .c.obj extensions
            if obj_file.endswith('.o'):
                su_file = obj_file[:-2] + '.su'
            elif obj_file.endswith('.c.obj'):
                su_file = obj_file[:-6] + '.c.su'
            else:
                continue

            if os.path.exists(su_file):
                su_files.append(su_file)

        if not su_files:
            print(f"{Colors.RED}‚ùå Error: No stack usage files (.su) found.{Colors.END}")
            print(f"{Colors.YELLOW}üí° Please compile with -fstack-usage flag to generate .su files.{Colors.END}")
            print(f"{Colors.YELLOW}üí° Add '-fstack-usage' to your CMakeLists.txt target_compile_options.{Colors.END}")
            sys.exit(1)

        self.logger.info(f"  ‚úì Found {len(su_files)} stack usage files (from {len(obj_files)} object files)")

        # Check for RTL dump files for function pointer detection
        rtl_files = self._find_rtl_files(obj_files)
        if rtl_files:
            self.rtl_available = True
            self.logger.info(f"  ‚úì Found {len(rtl_files)} RTL dump files for function pointer detection")
        else:
            self.logger.info("  ‚ö† No RTL dump files found. Function pointer detection disabled.")
            self.logger.info("  üí° Compile with -fdump-rtl-dfinish for enhanced analysis.")

        return obj_files

    def parse_object_files(self, obj_files: List[str]) -> None:
        """
        Parse all object files to extract call graph and stack usage information.

        For each object file:
        - Disassembles to find function calls and build call graph
        - Reads corresponding .su file for stack frame sizes
        - Analyzes RTL dumps for function pointer detection (if available)

        Args:
            obj_files: List of object file paths to process
        """
        self.logger.info(f"üìÅ Parsing {len(obj_files)} object files...")

        for i, obj_file in enumerate(obj_files, 1):
            # Progress indicator for large projects
            if i % 50 == 0 or i == len(obj_files):
                self.logger.debug(f"Progress: {i}/{len(obj_files)} files processed")

            self._parse_disassembly(obj_file)
            self._parse_su_file(obj_file)

        # Parse RTL dumps for function pointer detection if available
        if self.rtl_available:
            self._parse_rtl_files(obj_files)

        self.logger.info("‚úì Parsing complete")

    def _parse_disassembly(self, obj_file: str) -> None:
        """
        Parse objdump disassembly output to build function call graph.

        Extracts:
        - Function definitions and their addresses
        - Function call relationships via relocation entries
        - Symbol address mappings for resolution

        Args:
            obj_file: Path to object file to disassemble

        Raises:
            SystemExit: If disassembly fails
        """
        try:
            result = subprocess.run([self.objdump_path, "-dr", obj_file],
                                    capture_output=True, text=True, check=True)

            current_function = None

            for line in result.stdout.splitlines():
                line = line.strip()

                # Match function definition: "00000000 <function_name>:"
                func_match = re.match(r'^([0-9a-fA-F]+) <(.*)>:', line)
                if func_match:
                    addr, name = func_match.groups()
                    current_function = f"{name}@{obj_file}"
                    self.call_graph[current_function] = set()

                    # Track ambiguous function names across object files
                    if name in self.global_name:
                        self.ambiguous.add(name)
                    self.global_name[name] = current_function

                    # Store address mapping for symbol resolution
                    addr = addr.lstrip('0') or '0'
                    self.addresses[f"{addr}@{obj_file}"] = current_function

                # Match function call: ": R_ARM_CALL target"
                call_match = re.search(r': R_[A-Za-z0-9_]+_CALL\s+(.+)', line)
                if call_match and current_function:
                    target = call_match.group(1)

                    # Handle different call target formats
                    if target == ".text":
                        target = f"@{obj_file}"
                    elif target.startswith(".text+0x"):
                        offset = target[9:]  # Remove ".text+0x" prefix
                        target = f"{offset}@{obj_file}"

                    self.call_graph[current_function].add(target)

        except subprocess.CalledProcessError as e:
            print(f"{Colors.RED}‚ùå Error disassembling {obj_file}: {e}{Colors.END}")
            sys.exit(1)

    def _parse_su_file(self, obj_file: str) -> None:
        """
        Parse stack usage (.su) file to extract function frame sizes.

        The .su file format is: "path:line:column:function_name size qualifier"
        Frame sizes are adjusted by adding the ARM call overhead cost.

        Args:
            obj_file: Object file path (corresponding .su file will be located)
        """
        # Handle both .o and .c.obj extensions
        if obj_file.endswith('.o'):
            su_file = obj_file[:-2] + '.su'
        elif obj_file.endswith('.c.obj'):
            su_file = obj_file[:-6] + '.c.su'
        else:
            return

        try:
            with open(su_file, 'r', encoding='utf-8') as f:
                for line in f:
                    # Parse: "/path/file.c:137:20:function_name   size   qualifier"
                    match = re.search(r':([^:\t ]+)[\t ]+([0-9]+)[\t ]+', line)
                    if match:
                        func_name, size = match.groups()
                        # Add ARM function call overhead to frame size
                        self.frame_size[f"{func_name}@{obj_file}"] = int(size) + CALL_COST

        except FileNotFoundError:
            # Stack usage files may not exist for assembly files or system libraries
            pass

    def _parse_rtl_files(self, obj_files: List[str]) -> None:
        """
        Parse RTL dump files to detect function pointer calls.

        RTL dumps contain detailed information about function calls including
        indirect calls via function pointers, which make stack analysis unbounded.

        Args:
            obj_files: List of object file paths to find corresponding RTL files
        """
        self.logger.info("üîç Analyzing RTL dumps for function pointer detection...")

        rtl_files = self._find_rtl_files(obj_files)
        functions_with_pointers = 0

        for rtl_file in rtl_files:
            try:
                self._parse_single_rtl_file(rtl_file)
            except Exception as e:
                if self.show_warnings:
                    self.logger.warning(f"‚ö†Ô∏è Error parsing RTL file {rtl_file}: {e}")

        functions_with_pointers = len(self.has_function_pointer)
        if functions_with_pointers > 0:
            self.logger.info(f"  ‚úì Detected {functions_with_pointers} functions with function pointer calls")
        else:
            self.logger.info("  ‚úì No function pointer calls detected")

    def _parse_single_rtl_file(self, rtl_file: str) -> None:
        """
        Parse a single RTL dump file to detect function pointer calls.

        RTL format analysis:
        - Function definitions: ";; Function function_name (symbol_name, funcdef_no=N)"
        - Direct calls: '(call ... "function_name" ...)'
        - Indirect calls: '(call ...' without quoted function name

        Args:
            rtl_file: Path to RTL dump file
        """
        # Extract object file path from RTL file for function mapping
        if '.c.' in rtl_file and rtl_file.endswith('.dfinish'):
            # Convert RTL filename back to object file
            base_name = rtl_file.split('.c.')[0]
            obj_file = base_name + '.o'
            if not os.path.exists(obj_file):
                obj_file = base_name + '.c.obj'
        else:
            return

        current_function = None

        # Regex patterns for RTL analysis
        function_pattern = re.compile(r'^;; Function (.*) \((\S+), funcdef_no=\d+.*\).*$')
        # Direct calls have symbol_ref with quoted function names
        direct_call_pattern = re.compile(r'.*\(call.*symbol_ref.*"([^"]+)".*\).*')
        # Indirect calls use registers instead of symbol_ref
        indirect_call_pattern = re.compile(r'.*\(call.*\(mem:SI \(reg.*\).*\).*')

        try:
            with open(rtl_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()

                    # Match function definition
                    func_match = function_pattern.match(line)
                    if func_match:
                        symbol_name = func_match.group(2)
                        current_function = f"{symbol_name}@{obj_file}"
                        continue

                    # Skip if no current function context
                    if not current_function:
                        continue

                    # Check for direct function calls (these are safe)
                    if direct_call_pattern.match(line):
                        continue

                    # Check for indirect function calls (function pointers)
                    if indirect_call_pattern.match(line):
                        self.has_function_pointer.add(current_function)
                        if self.show_warnings:
                            func_name = current_function.split('@')[0]
                            self.logger.debug(f"  Function pointer call detected in {func_name}")

        except FileNotFoundError:
            if self.show_warnings:
                self.logger.warning(f"‚ö†Ô∏è RTL file not found: {rtl_file}")
        except Exception as e:
            if self.show_warnings:
                self.logger.warning(f"‚ö†Ô∏è Error reading RTL file {rtl_file}: {e}")

    def resolve_symbols(self) -> None:
        """
        Resolve function call targets using symbol tables and address mappings.

        This process converts raw call targets (addresses, offsets, names) into
        fully qualified function references that can be used for call graph analysis.

        Resolution priority:
        1. Direct address matches
        2. Global symbol name matches
        3. Existing call graph entries
        4. Mark as unresolved
        """
        self.logger.info("üîó Resolving symbols...")

        for func_name, callees in self.call_graph.items():
            resolved = set()

            for target in callees:
                if target in self.addresses:
                    # Direct address match
                    resolved.add(self.addresses[target])
                elif target in self.global_name:
                    # Global symbol name match
                    resolved.add(self.global_name[target])
                    if target in self.ambiguous and self.show_warnings:
                        self.logger.warning(f"‚ö†Ô∏è  Ambiguous resolution: {target}")
                elif target in self.call_graph:
                    # Already in call graph
                    resolved.add(target)
                else:
                    # Cannot resolve - likely external library function
                    self.unresolved.add(target)

            self.call_graph[func_name] = resolved

    def create_interrupt_node(self) -> None:
        """
        Create virtual interrupt node for interrupt service routine analysis.

        This creates a synthetic "INTERRUPT" node that calls all interrupt handlers
        (functions matching "__vector_*" pattern). This allows calculation of
        worst-case interrupt stack usage.
        """
        self.call_graph["INTERRUPT"] = set()

        for func_name in self.call_graph:
            if "__vector_" in func_name:
                self.call_graph["INTERRUPT"].add(func_name)

    def trace_call_graph(self, quiet: bool = False) -> None:
        """
        Traverse the call graph to calculate stack costs and call depths.

        Uses depth-first traversal with memoization to compute:
        - Total stack cost: frame size + maximum callee cost
        - Call depth: maximum depth of any call path
        - Recursion detection: marks recursive functions

        The algorithm handles cycles by detecting when a function is revisited
        during traversal and marking it as recursive.
        """
        self.logger.info("üìä Analyzing call graph...")

        def trace(func_name: str) -> None:
            """Recursive traversal function with cycle detection"""
            if func_name in self.visited:
                if self.visited[func_name] == "?":
                    # Cycle detected - mark as recursive
                    self.visited[func_name] = "R"
                return

            # Mark as currently being processed
            self.visited[func_name] = "?"

            # Check if function has function pointer calls (unbounded stack)
            if func_name in self.has_function_pointer:
                self.visited[func_name] = "P"  # Mark as function pointer
                self.total_cost[func_name] = float('inf')  # Unbounded
                self.call_depth[func_name] = float('inf')
                return

            max_depth = 0
            max_frame = 0

            # Process all callees
            if func_name in self.call_graph:
                for callee in self.call_graph[func_name]:
                    self.has_caller.add(callee)
                    trace(callee)

                    callee_cost = self.total_cost.get(callee, 0)
                    callee_depth = self.call_depth.get(callee, 0)

                    # If any callee is unbounded, this function is unbounded
                    if callee_cost == float('inf') or callee_depth == float('inf'):
                        self.visited[func_name] = "U"  # Mark as unbounded
                        self.total_cost[func_name] = float('inf')
                        self.call_depth[func_name] = float('inf')
                        return

                    # Track maximum cost and depth among callees
                    max_frame = max(max_frame, callee_cost)
                    max_depth = max(max_depth, callee_depth)

            # Calculate final values
            self.call_depth[func_name] = max_depth + 1
            self.total_cost[func_name] = max_frame + self.frame_size.get(func_name, 0)

            # Mark as completed (if not recursive or unbounded)
            if self.visited[func_name] == "?":
                self.visited[func_name] = " "

        # Process all functions in the call graph
        for func_name in self.call_graph:
            trace(func_name)

    def generate_report(self) -> None:
        """
        Generate and display comprehensive stack usage analysis report.

        The report includes:
        - File-based stack usage summary
        - Detailed function table with costs and depths
        - Peak stack usage estimates
        - Unresolved function warnings
        """
        print(f"\n{Colors.BOLD}{Colors.HEADER}{'='*80}{Colors.END}")
        print(f"{Colors.BOLD}{Colors.HEADER}üìà STACK USAGE ANALYSIS REPORT{Colors.END}")
        print(f"{Colors.BOLD}{Colors.HEADER}{'='*80}{Colors.END}\n")

        # Prepare function data and file statistics
        functions = []
        file_stats = defaultdict(lambda: {'count': 0, 'total_cost': 0, 'max_cost': 0, 'functions': []})
        max_iv_cost = 0
        main_cost = 0

        for func_full_name in sorted(self.total_cost.keys(),
                                     key=lambda x: self.total_cost[x], reverse=True):

            # Extract clean function name and source file
            display_name = func_full_name
            source_file = "unknown"

            if '@' in func_full_name:
                base_name = func_full_name.split('@')[0]
                obj_file = func_full_name.split('@')[1]

                # Extract source file name from object file path
                if '/' in obj_file:
                    source_file = os.path.basename(obj_file).replace('.o', '')
                else:
                    source_file = obj_file.replace('.o', '')

                # Use clean name if not ambiguous
                if base_name not in self.ambiguous:
                    display_name = base_name

            # Determine function characteristics
            visit_status = self.visited.get(func_full_name, ' ')
            is_recursive = visit_status == 'R'
            is_root = func_full_name not in self.has_caller
            cost = self.total_cost[func_full_name]

            # Determine unbounded status and reason
            is_unbounded = cost == float('inf')
            unbounded_reason = None
            if is_unbounded:
                if visit_status == 'R':
                    unbounded_reason = 'recursion'
                elif visit_status == 'P':
                    unbounded_reason = 'function_pointer'
                elif visit_status == 'U':
                    unbounded_reason = 'unbounded_callee'
                else:
                    unbounded_reason = 'unknown'

            # Track special functions for summary (skip unbounded functions)
            if not is_unbounded:
                if "__vector_" in func_full_name:
                    max_iv_cost = max(max_iv_cost, cost)
                elif func_full_name.startswith("main@"):
                    main_cost = cost

            func_info = FunctionInfo(
                name=display_name,
                cost=cost,
                frame_size=self.frame_size.get(func_full_name, 0),
                height=self.call_depth.get(func_full_name, 0),
                is_recursive=is_recursive,
                is_root=is_root,
                is_unbounded=is_unbounded,
                unbounded_reason=unbounded_reason
            )

            functions.append(func_info)

            # Update file statistics
            if cost > 0:
                file_stats[source_file]['count'] += 1
                file_stats[source_file]['total_cost'] += cost
                file_stats[source_file]['max_cost'] = max(file_stats[source_file]['max_cost'], cost)
                file_stats[source_file]['functions'].append((display_name, cost, is_root))

        # Generate report sections
        self._print_file_hierarchy(file_stats)
        self._print_function_table(functions)
        self._print_summary(main_cost, max_iv_cost)
        self._print_unresolved()

        # Return data for potential JSON export
        return self._build_analysis_data(functions, file_stats, main_cost, max_iv_cost)

    def generate_analysis_data(self) -> Dict:
        """
        Generate analysis data without printing report (for JSON mode).

        Returns:
            Analysis data dictionary
        """
        # Prepare function data and file statistics (same logic as generate_report)
        functions = []
        file_stats = defaultdict(lambda: {'count': 0, 'total_cost': 0, 'max_cost': 0, 'functions': []})
        max_iv_cost = 0
        main_cost = 0

        for func_full_name in sorted(self.total_cost.keys(),
                                     key=lambda x: self.total_cost[x], reverse=True):

            # Extract clean function name and source file
            display_name = func_full_name
            source_file = "unknown"

            if '@' in func_full_name:
                base_name = func_full_name.split('@')[0]
                obj_file = func_full_name.split('@')[1]

                # Extract source file name from object file path
                if '/' in obj_file:
                    source_file = os.path.basename(obj_file).replace('.o', '').replace('.c.obj', '.cbj')
                else:
                    source_file = obj_file.replace('.o', '').replace('.c.obj', '.cbj')

                # Use clean name if not ambiguous
                if base_name not in self.ambiguous:
                    display_name = base_name

            # Determine function characteristics
            visit_status = self.visited.get(func_full_name, ' ')
            is_recursive = visit_status == 'R'
            is_root = func_full_name not in self.has_caller
            cost = self.total_cost[func_full_name]

            # Determine unbounded status and reason
            is_unbounded = cost == float('inf')
            unbounded_reason = None
            if is_unbounded:
                if visit_status == 'R':
                    unbounded_reason = 'recursion'
                elif visit_status == 'P':
                    unbounded_reason = 'function_pointer'
                elif visit_status == 'U':
                    unbounded_reason = 'unbounded_callee'
                else:
                    unbounded_reason = 'unknown'

            # Track special functions for summary (skip unbounded functions)
            if not is_unbounded:
                if "__vector_" in func_full_name:
                    max_iv_cost = max(max_iv_cost, cost)
                elif func_full_name.startswith("main@"):
                    main_cost = cost

            func_info = FunctionInfo(
                name=display_name,
                cost=cost,
                frame_size=self.frame_size.get(func_full_name, 0),
                height=self.call_depth.get(func_full_name, 0),
                is_recursive=is_recursive,
                is_root=is_root,
                is_unbounded=is_unbounded,
                unbounded_reason=unbounded_reason
            )

            functions.append(func_info)

            # Update file statistics
            if cost > 0:
                file_stats[source_file]['count'] += 1
                file_stats[source_file]['total_cost'] += cost
                file_stats[source_file]['max_cost'] = max(file_stats[source_file]['max_cost'], cost)
                file_stats[source_file]['functions'].append((display_name, cost, is_root))

        return self._build_analysis_data(functions, file_stats, main_cost, max_iv_cost)

    def generate_short_report(self) -> None:
        """
        Generate a short report with only function names and stack usage.

        Outputs a simple sorted list of functions with their stack costs.
        """
        # Collect functions with their costs
        function_costs = []

        for func_full_name in self.total_cost.keys():
            cost = self.total_cost[func_full_name]
            if cost >= self.threshold or cost == float('inf'):
                # Extract clean function name
                display_name = func_full_name
                if '@' in func_full_name:
                    base_name = func_full_name.split('@')[0]
                    # Use clean name if not ambiguous
                    if base_name not in self.ambiguous:
                        display_name = base_name

                cost_display = "unbounded" if cost == float('inf') else str(cost)
                function_costs.append((display_name, cost, cost_display))

        # Sort by cost (unbounded first, then descending)
        function_costs.sort(key=lambda x: (0 if x[1] == float('inf') else 1, -x[1] if x[1] != float('inf') else 0))

        # Output the list
        for func_name, _, cost_display in function_costs:
            print(f"{func_name}: {cost_display}")

    def _build_analysis_data(
            self,
            functions: List[FunctionInfo],
            file_stats: Dict,
            main_cost: int,
            max_iv_cost: int) -> Dict:
        """
        Build the analysis data dictionary.

        Args:
            functions: List of function information
            file_stats: File statistics dictionary
            main_cost: Main function cost
            max_iv_cost: Maximum interrupt cost

        Returns:
            Analysis data dictionary
        """
        metadata = {
            'timestamp': datetime.now().isoformat(),
            'build_dir': str(self.build_dir),
            'threshold': self.threshold,
            'total_functions': len(functions),
            'functions_analyzed': len([f for f in functions if f.cost > 0]),
            'rtl_analysis_enabled': self.rtl_available
        }

        # Add git information if available
        if self.git_info:
            metadata['git'] = self.git_info.copy()

        return {
            'metadata': metadata,
            'summary': {
                'main_cost': main_cost,
                'max_interrupt_cost': max_iv_cost,
                'total_peak_estimate': main_cost + max_iv_cost,
                'max_single_function': max((f.cost for f in functions), default=0),
                'average_stack_usage': sum(f.cost for f in functions) // len(functions) if functions else 0
            },
            'functions': [
                {
                    'name': func.name,
                    'total_cost': func.cost if not func.is_unbounded else 'unbounded',
                    'frame_size': func.frame_size,
                    'call_depth': func.height if func.height != float('inf') else 'unbounded',
                    'is_recursive': func.is_recursive,
                    'is_root': func.is_root,
                    'is_unbounded': func.is_unbounded,
                    'unbounded_reason': func.unbounded_reason
                }
                for func in functions if func.cost > 0 or func.is_unbounded
            ],
            'files': {
                filename: {
                    'function_count': stats['count'],
                    'total_cost': stats['total_cost'],
                    'max_cost': stats['max_cost'],
                    'functions': [
                        {'name': name, 'cost': cost, 'is_root': is_root}
                        for name, cost, is_root in stats['functions']
                    ]
                }
                for filename, stats in file_stats.items()
                if stats['total_cost'] > 0
            },
            'unresolved': sorted(list(self.unresolved))
        }

    def _print_file_hierarchy(self, file_stats: Dict) -> None:
        """
        Display stack usage organized by source file.

        Shows top functions per file and total usage statistics.

        Args:
            file_stats: Dictionary mapping filenames to usage statistics
        """
        print(f"{Colors.BOLD}üìÅ Stack Usage by Source File{Colors.END}\n")

        # Sort files by total stack usage (descending)
        sorted_files = sorted(file_stats.items(),
                              key=lambda x: x[1]['total_cost'], reverse=True)

        # Display files with non-zero stack usage
        for filename, stats in sorted_files:
            if stats['total_cost'] > 0:
                # Show top 3 functions per file
                top_funcs = sorted(stats['functions'], key=lambda x: x[1], reverse=True)[:3]

                print(f"{filename}")
                for name, cost, is_root in top_funcs:
                    print(f"  - {name} ({cost} bytes)")
                print(f"  Total: {stats['total_cost']} bytes ({stats['count']} functions)\n")

    def _print_function_table(self, functions: List[FunctionInfo]) -> None:
        """
        Display detailed function stack usage table.

        Shows functions above the threshold with their costs, frame sizes,
        and call depths. Functions are sorted by total cost (descending).

        Args:
            functions: List of function information objects
        """
        # Apply threshold filter (include unbounded functions)
        filtered_functions = [f for f in functions if f.cost >= self.threshold or f.is_unbounded]

        if not filtered_functions:
            print(f"{Colors.YELLOW}‚ö†Ô∏è  No functions with significant stack usage found{Colors.END}")
            return

        print(f"{Colors.BOLD}üìã Function Stack Usage Analysis{Colors.END}")
        threshold_msg = (f"   (Showing {len(filtered_functions)} functions with stack usage "
                         f"‚â•{self.threshold} bytes or unbounded)")
        print(f"{Colors.CYAN}{threshold_msg}{Colors.END}")
        frame_msg = "   Total = Frame + Max(Callees), Frame = Local variables, Depth = Call chain length"
        print(f"{Colors.CYAN}{frame_msg}{Colors.END}")
        symbols_msg = "   Symbols: ‚ñ∂=Root, üîÑ=Recursive, ‚ö†=Function pointer, ‚ùå=Unbounded"
        print(f"{Colors.CYAN}{symbols_msg}{Colors.END}\n")

        # Table header with proper alignment
        header = f"{'Flag':<5}{'Function Name':<40} {'Total':<9} {'Frame':<7} {'Depth':<5}"
        print(f"{Colors.BOLD}{Colors.UNDERLINE}{header}{Colors.END}")
        separator = (f"{'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<5}{'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<40} "
                     f"{'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<9} {'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<7} {'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<5}")
        print(f"{Colors.BOLD}{Colors.UNDERLINE}{separator}{Colors.END}")

        # Display function rows
        for func in filtered_functions:
            self._print_function_row(func, Colors.END)

        # Summary information
        very_low_cost = len([f for f in functions if f.cost < self.threshold and f.cost > 0])
        if very_low_cost > 0:
            low_cost_msg = (f"\n{Colors.CYAN}üí≠ {very_low_cost} functions with very low cost "
                            f"(<{self.threshold} bytes) hidden for clarity{Colors.END}")
            print(low_cost_msg)

        displayed_count = len(filtered_functions)
        total_with_usage = len([f for f in functions if f.cost > 0])
        summary_msg = (f"\n{Colors.BOLD}üìä Displayed {displayed_count} of {total_with_usage} "
                       f"functions with stack usage{Colors.END}")
        print(summary_msg)

    def _print_function_row(self, func: FunctionInfo, color: str) -> None:
        """
        Print a single function row in the analysis table.

        Uses consistent formatting with visual indicators for function types:
        - ‚ñ∂ for root functions (entry points)
        - üîÑ for recursive functions
        - ‚ö† for functions with function pointer calls
        - ‚ùå for unbounded functions (other reasons)
        - Color coding based on stack cost

        Args:
            func: Function information object
            color: Base color for the row
        """
        # Determine function type indicator with consistent spacing
        if func.is_unbounded:
            if func.unbounded_reason == 'recursion':
                flag = f"{Colors.RED}üîÑ{Colors.END}  "  # Recursive function
            elif func.unbounded_reason == 'function_pointer':
                flag = f"{Colors.YELLOW}‚ö†{Colors.END}   "  # Function pointer call
            else:
                flag = f"{Colors.RED}‚ùå{Colors.END}  "  # Other unbounded reason
        elif func.is_root:
            flag = f"{Colors.GREEN}‚ñ∂{Colors.END}   "  # Root function
        else:
            flag = "     "  # Regular function

        # Apply cost-based color coding
        cost_display = str(func.cost)

        if func.is_unbounded:
            cost_display = "unbounded"
        # Color coding is applied directly in the row formatting below

        # Handle long function names
        display_name = func.name
        if len(display_name) > 39:
            display_name = display_name[:36] + "..."

        # Handle unbounded depth display
        depth_display = str(func.height) if func.height != float('inf') else "‚àû"

        # Print formatted row with consistent alignment
        row = (f"{display_name:<40} {cost_display:<9} {func.get('frame_size', 0):<7} "
               f"{depth_display:<5} {flag}")
        print(row)

    def _print_summary(self, main_cost: int, max_iv_cost: int) -> None:
        """
        Display analysis summary with peak stack usage estimates.

        Provides estimates for:
        - Main execution path stack usage
        - Worst-case interrupt handler stack usage
        - Combined peak usage estimate
        - General statistics

        Args:
            main_cost: Stack cost of main function
            max_iv_cost: Maximum interrupt handler cost
        """
        print(f"\n{Colors.BOLD}{Colors.HEADER}üìä ANALYSIS SUMMARY{Colors.END}")
        print("‚îÄ" * 50)

        interrupt_cost = self.total_cost.get("INTERRUPT", 0)
        total_peak = main_cost + max_iv_cost

        print("üéØ Peak execution estimate:")
        print(f"   Main function cost:     {Colors.CYAN}{main_cost:>6}{Colors.END} bytes")
        print(f"   Worst interrupt cost:   {Colors.YELLOW}{max_iv_cost:>6}{Colors.END} bytes")
        print(f"   Virtual interrupt node: {Colors.BLUE}{interrupt_cost:>6}{Colors.END} bytes")
        total_msg = (f"   {Colors.BOLD}Total estimated peak:   "
                     f"{Colors.GREEN}{total_peak:>6}{Colors.END}{Colors.BOLD} bytes{Colors.END}")
        print(total_msg)

        # Additional statistics
        all_costs = list(self.total_cost.values())
        if all_costs:
            print("\nüìà Statistics:")
            print(f"   Functions analyzed:     {Colors.CYAN}{len(all_costs):>6}{Colors.END}")
            print(f"   Average stack usage:    {Colors.YELLOW}{sum(all_costs)//len(all_costs):>6}{Colors.END} bytes")
            print(f"   Maximum single function:{Colors.RED}{max(all_costs):>6}{Colors.END} bytes")

    def _print_unresolved(self) -> None:
        """
        Display list of unresolved function references.

        These are typically external library functions or system calls
        that don't have corresponding object files in the build directory.
        """
        if self.unresolved:
            print(f"\n{Colors.YELLOW}{Colors.BOLD}‚ö†Ô∏è  UNRESOLVED FUNCTIONS{Colors.END}")
            print("‚îÄ" * 30)
            print(f"{Colors.YELLOW}The following functions could not be resolved:{Colors.END}")

            # Display limited list to avoid overwhelming output
            sorted_unresolved = sorted(self.unresolved)
            for i, func in enumerate(sorted_unresolved):
                if i < 20:
                    print(f"   ‚Ä¢ {func}")
                elif i == 20:
                    print(f"   ‚Ä¢ ... and {len(sorted_unresolved) - 20} more")
                    break

    def export_json(self, output_file: str, data: Dict) -> None:
        """
        Export analysis data to JSON format.

        Args:
            output_file: Path to output JSON file
            data: Analysis data dictionary to export
        """
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, sort_keys=True)
            print(f"\n{Colors.GREEN}‚úì Analysis data exported to {output_file}{Colors.END}")
        except Exception as e:
            print(f"{Colors.RED}‚ùå Error exporting JSON: {e}{Colors.END}")


def load_json_data(file_path: str) -> Optional[Dict]:
    """
    Load analysis data from JSON file.

    Args:
        file_path: Path to JSON file

    Returns:
        Analysis data dictionary or None if failed
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"{Colors.RED}‚ùå Error loading {file_path}: {e}{Colors.END}")
        return None


def read_and_display_json(json_file: str, threshold: int = 0, output_format: str = 'table') -> None:
    """
    Read JSON analysis file and display it, optionally filtering by threshold.

    Args:
        json_file: Path to JSON analysis file
        threshold: Minimum stack usage threshold in bytes (0 = show all)
        output_format: Output format ('table', 'json', 'short')
    """
    data = load_json_data(json_file)
    if not data:
        return

    # Get all functions or filter by threshold
    all_functions = data.get('functions', [])
    if threshold > 0:
        # Filter functions by threshold
        filtered_functions = []
        for func in all_functions:
            cost = func.get('total_cost')
            if cost == 'unbounded' or (isinstance(cost, (int, float)) and cost >= threshold):
                filtered_functions.append(func)
        functions_to_show = filtered_functions
        is_filtered = True
    else:
        functions_to_show = all_functions
        is_filtered = False

    if output_format == 'json':
        # Output data as JSON
        if is_filtered:
            filtered_data = data.copy()
            filtered_data['functions'] = functions_to_show
            filtered_data['metadata']['filtered_threshold'] = threshold
            filtered_data['metadata']['filtered_count'] = len(functions_to_show)
            print(json.dumps(filtered_data, indent=2, sort_keys=True))
        else:
            print(json.dumps(data, indent=2, sort_keys=True))

    elif output_format == 'short':
        # Simple list format
        for func in sorted(functions_to_show, key=lambda x: 0 if x['total_cost'] == 'unbounded' else -x['total_cost']):
            cost_display = func['total_cost'] if func['total_cost'] == 'unbounded' else str(func['total_cost'])
            print(f"{func['name']}: {cost_display}")

    else:
        # Table format (default)
        title = "üìä FILTERED STACK USAGE ANALYSIS" if is_filtered else "üìä STACK USAGE ANALYSIS"
        print(f"{Colors.BOLD}{Colors.HEADER}{title}{Colors.END}")
        print(f"{Colors.CYAN}Source: {json_file}{Colors.END}")

        if is_filtered:
            print(f"{Colors.CYAN}Threshold: ‚â•{threshold} bytes or unbounded{Colors.END}")
            print(f"{Colors.CYAN}Showing: {len(functions_to_show)} of {len(all_functions)} functions{Colors.END}\n")
        else:
            print(f"{Colors.CYAN}Showing: {len(functions_to_show)} functions{Colors.END}\n")

        if not functions_to_show:
            if is_filtered:
                print(f"{Colors.YELLOW}‚ö†Ô∏è  No functions found with stack usage ‚â•{threshold} bytes{Colors.END}")
            else:
                print(f"{Colors.YELLOW}‚ö†Ô∏è  No functions found in analysis{Colors.END}")
            return

        # Display metadata summary
        metadata = data.get('metadata', {})
        if metadata:
            print(f"{Colors.BOLD}üìã Analysis Metadata:{Colors.END}")
            if 'timestamp' in metadata:
                print(f"  Analysis time: {metadata['timestamp']}")
            if 'build_dir' in metadata:
                print(f"  Build directory: {metadata['build_dir']}")
            if 'total_functions' in metadata:
                print(f"  Total functions: {metadata['total_functions']}")
            if 'rtl_analysis_enabled' in metadata:
                rtl_status = "enabled" if metadata['rtl_analysis_enabled'] else "disabled"
                print(f"  RTL analysis: {rtl_status}")
            if 'git' in metadata and metadata['git']:
                git_info = metadata['git']
                print(f"  Git: {git_info.get('branch', 'unknown')}@{git_info.get('commit_hash_short', 'unknown')}")
            print()

        # Table header
        header = f"{'Function Name':<40} {'Total':<9} {'Frame':<7} {'Depth':<5} {'Type':<12}"
        print(f"{Colors.BOLD}{Colors.UNDERLINE}{header}{Colors.END}")
        separator = (f"{'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<40} "
                     f"{'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<9} {'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<7} {'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<5} "
                     f"{'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<12}")
        print(f"{Colors.BOLD}{Colors.UNDERLINE}{separator}{Colors.END}")

        # Sort by cost (unbounded first, then descending)
        sorted_functions = sorted(functions_to_show, key=lambda x: (
            0 if x['total_cost'] == 'unbounded' else 1, -(x['total_cost'] if x['total_cost'] != 'unbounded' else 0)))

        for func in sorted_functions:
            # Determine function type and color
            if func.get('is_unbounded', False):
                if func.get('unbounded_reason') == 'recursion':
                    type_indicator = f"{Colors.RED}üîÑ Recursive{Colors.END}"
                elif func.get('unbounded_reason') == 'function_pointer':
                    type_indicator = f"{Colors.YELLOW}‚ö† Func Ptr{Colors.END}"
                else:
                    type_indicator = f"{Colors.RED}‚ùå Unbounded{Colors.END}"
                cost_display = f"{Colors.RED}{Colors.BOLD}unbounded{Colors.END}"
            else:
                type_indicator = f"{Colors.GREEN}‚ñ∂ Root{Colors.END}" if func.get('is_root', False) else "  Normal"
                cost = func['total_cost']
                if cost >= 500:
                    cost_display = f"{Colors.RED}{Colors.BOLD}{cost}{Colors.END}"
                elif cost >= 200:
                    cost_display = f"{Colors.RED}{cost}{Colors.END}"
                elif cost >= 100:
                    cost_display = f"{Colors.YELLOW}{cost}{Colors.END}"
                else:
                    cost_display = f"{Colors.CYAN}{cost}{Colors.END}"

            # Handle long function names
            display_name = func['name']
            if len(display_name) > 39:
                display_name = display_name[:36] + "..."

            depth_display = str(func.get('call_depth', 0)) if func.get('call_depth') != 'unbounded' else "‚àû"

            row = (f"{display_name:<40} {cost_display:<9} {func.get('frame_size', 0):<7} "
                   f"{depth_display:<5} {type_indicator}")
            print(row)

        # Summary
        print(f"\n{Colors.BOLD}üìà Summary:{Colors.END}")
        unbounded_count = len([f for f in functions_to_show if f.get('is_unbounded', False)])
        bounded_count = len(functions_to_show) - unbounded_count
        print(f"  Unbounded functions: {Colors.RED}{unbounded_count}{Colors.END}")
        print(f"  Bounded functions:   {Colors.GREEN}{bounded_count}{Colors.END}")
        print(f"  Total shown:         {Colors.CYAN}{len(functions_to_show)}{Colors.END}")

        if is_filtered and len(functions_to_show) < len(all_functions):
            hidden_count = len(all_functions) - len(functions_to_show)
            print(f"  Hidden (< threshold): {Colors.YELLOW}{hidden_count}{Colors.END}")


def compare_analyses(old_data: Dict, new_data: Dict) -> None:
    """
    Compare two stack analysis results and display git-diff like output.

    Args:
        old_data: Previous analysis data
        new_data: Current analysis data
    """
    print(f"\n{Colors.BOLD}{Colors.HEADER}{'='*80}{Colors.END}")
    print(f"{Colors.BOLD}{Colors.HEADER}üìä STACK USAGE COMPARISON{Colors.END}")
    print(f"{Colors.BOLD}{Colors.HEADER}{'='*80}{Colors.END}\n")

    # Compare metadata
    old_time = old_data.get('metadata', {}).get('timestamp', 'unknown')
    new_time = new_data.get('metadata', {}).get('timestamp', 'unknown')
    print(f"{Colors.CYAN}Comparing:{Colors.END}")
    print(f"  Old: {old_time}")
    print(f"  New: {new_time}")

    # Compare summary statistics
    old_summary = old_data.get('summary', {})
    new_summary = new_data.get('summary', {})

    print(f"\n{Colors.BOLD}üìà Summary Changes:{Colors.END}")
    _print_summary_diff('Peak estimate', old_summary.get('total_peak_estimate', 0),
                        new_summary.get('total_peak_estimate', 0))
    _print_summary_diff('Max single function', old_summary.get('max_single_function', 0),
                        new_summary.get('max_single_function', 0))
    _print_summary_diff('Average usage', old_summary.get('average_stack_usage', 0),
                        new_summary.get('average_stack_usage', 0))

    # Compare functions
    old_functions = {f['name']: f for f in old_data.get('functions', [])}
    new_functions = {f['name']: f for f in new_data.get('functions', [])}

    all_functions = set(old_functions.keys()) | set(new_functions.keys())
    changes = []

    for func_name in sorted(all_functions):
        old_func = old_functions.get(func_name)
        new_func = new_functions.get(func_name)

        if old_func and new_func:
            old_cost = old_func['total_cost']
            new_cost = new_func['total_cost']
            if old_cost != new_cost:
                diff = new_cost - old_cost
                changes.append((func_name, old_cost, new_cost, diff, 'modified'))
        elif old_func and not new_func:
            changes.append((func_name, old_func['total_cost'], 0, -old_func['total_cost'], 'removed'))
        elif not old_func and new_func:
            changes.append((func_name, 0, new_func['total_cost'], new_func['total_cost'], 'added'))

    if changes:
        print(f"\n{Colors.BOLD}üîÑ Function Changes:{Colors.END}")
        header = f"{'Status':<8} {'Function Name':<40} {'Old':<7} {'New':<7} {'Diff':<8}"
        print(f"{Colors.BOLD}{Colors.UNDERLINE}{header}{Colors.END}")
        separator = (f"{'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<8} {'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<40} "
                     f"{'‚îÄ‚îÄ‚îÄ':<7} {'‚îÄ‚îÄ‚îÄ':<7} {'‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ':<8}")
        print(f"{Colors.BOLD}{Colors.UNDERLINE}{separator}{Colors.END}")

        # Sort by absolute difference (largest changes first)
        changes.sort(key=lambda x: abs(x[3]), reverse=True)

        for func_name, old_cost, new_cost, diff, status in changes[:20]:  # Show top 20 changes
            _print_function_diff(func_name, old_cost, new_cost, diff, status)

        if len(changes) > 20:
            print(f"\n{Colors.CYAN}üí≠ {len(changes) - 20} more changes not shown{Colors.END}")
    else:
        print(f"\n{Colors.GREEN}‚úì No function-level changes detected{Colors.END}")

    # Compare file statistics
    old_files = old_data.get('files', {})
    new_files = new_data.get('files', {})

    file_changes = []
    all_files = set(old_files.keys()) | set(new_files.keys())

    for file_name in sorted(all_files):
        old_file = old_files.get(file_name, {})
        new_file = new_files.get(file_name, {})

        old_total = old_file.get('total_cost', 0)
        new_total = new_file.get('total_cost', 0)

        if old_total != new_total:
            diff = new_total - old_total
            file_changes.append((file_name, old_total, new_total, diff))

    if file_changes:
        print(f"\n{Colors.BOLD}üìÅ File-level Changes:{Colors.END}")
        file_changes.sort(key=lambda x: abs(x[3]), reverse=True)

        for file_name, old_total, new_total, diff in file_changes[:10]:
            _print_file_diff(file_name, old_total, new_total, diff)


def _print_summary_diff(label: str, old_val: int, new_val: int) -> None:
    """Print a summary statistic difference."""
    diff = new_val - old_val
    if diff == 0:
        print(f"  {label:<20}: {new_val:>6} bytes (no change)")
    else:
        color = Colors.RED if diff > 0 else Colors.GREEN
        sign = '+' if diff > 0 else ''
        summary_msg = (f"  {label:<20}: {old_val:>6} ‚Üí {color}{new_val:>6}{Colors.END} bytes "
                       f"({color}{sign}{diff:>+6}{Colors.END})")
        print(summary_msg)


def _print_function_diff(func_name: str, old_cost: int, new_cost: int, diff: int, status: str) -> None:
    """Print a function difference in git-diff style."""
    # Truncate long function names
    display_name = func_name[:39] + "..." if len(func_name) > 39 else func_name

    if status == 'added':
        add_msg = (f"{Colors.GREEN}+ ADD   {Colors.END} {display_name:<40} {'-':<7} "
                   f"{new_cost:<7} {Colors.GREEN}+{diff:<7}{Colors.END}")
        print(add_msg)
    elif status == 'removed':
        del_msg = (f"{Colors.RED}- DEL   {Colors.END} {display_name:<40} {old_cost:<7} "
                   f"{'-':<7} {Colors.RED}{diff:<8}{Colors.END}")
        print(del_msg)
    else:  # modified
        color = Colors.RED if diff > 0 else Colors.GREEN
        sign = '+' if diff > 0 else ''
        mod_msg = (f"{color}~ MOD   {Colors.END} {display_name:<40} {old_cost:<7} "
                   f"{new_cost:<7} {color}{sign}{diff:<7}{Colors.END}")
        print(mod_msg)


def _print_file_diff(file_name: str, old_total: int, new_total: int, diff: int) -> None:
    """Print a file-level difference."""
    color = Colors.RED if diff > 0 else Colors.GREEN
    sign = '+' if diff > 0 else ''
    file_msg = (f"  {file_name:<30}: {old_total:>6} ‚Üí {color}{new_total:>6}{Colors.END} bytes "
                f"({color}{sign}{diff:>+6}{Colors.END})")
    print(file_msg)


def main():
    """
    Main entry point for the stack analyzer tool.

    Handles command line argument parsing, tool initialization,
    and orchestrates the complete analysis workflow.
    """
    parser = argparse.ArgumentParser(
        description="Analyze stack usage of ARM embedded firmware",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python3 arm-stack build
  python3 arm-stack cmake-build-debug
  python3 arm-stack build --threshold 50
  python3 arm-stack build --no-color
  python3 arm-stack build --no-warnings
  python3 arm-stack build --export-json stack_usage.json
  python3 arm-stack build --json > stack_usage.json
  python3 arm-stack build --short
  python3 arm-stack build --objdump /path/to/arm-none-eabi-objdump
  python3 arm-stack --compare old.json new.json
  python3 arm-stack --read-json analysis.json
  python3 arm-stack --read-json analysis.json --threshold 100
  python3 arm-stack --read-json analysis.json --threshold 50 --json
  python3 arm-stack --read-json analysis.json --short
  python3 arm-stack --filter-json analysis.json --threshold 100  # deprecated

Function Pointer Detection:
  Compile with: gcc -c -fstack-usage -fdump-rtl-dfinish source.c
  Analyze with: python3 arm-stack build
        """
    )
    parser.add_argument('build_dir', nargs='?', help='Build directory containing object files')
    parser.add_argument('--threshold', type=int, default=10,
                        help='Minimum stack usage threshold in bytes (default: 10)')
    parser.add_argument('--no-color', action='store_true', help='Disable colored output')
    parser.add_argument('--no-warnings', action='store_true', help='Suppress warning messages')
    parser.add_argument('--export-json', type=str, metavar='FILE',
                        help='Export analysis data to JSON file')
    parser.add_argument('--json', action='store_true',
                        help='Output analysis results in JSON format to stdout')
    parser.add_argument('--short', action='store_true',
                        help='Output only a sorted list of functions with stack usage')
    parser.add_argument('--objdump', type=str, default=DEFAULT_OBJDUMP, metavar='PATH',
                        help=f'Path to objdump executable (default: {DEFAULT_OBJDUMP})')
    parser.add_argument('--compare', type=str, nargs=2, metavar=('OLD', 'NEW'),
                        help='Compare two JSON analysis files')
    parser.add_argument('--read-json', type=str, metavar='FILE',
                        help='Read and display JSON analysis file (can be combined with --threshold for filtering)')
    parser.add_argument(
        '--filter-json',
        type=str,
        metavar='FILE',
        help='Read JSON file and filter functions by stack usage threshold (deprecated: use --read-json)')
    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__} by {__author__} ({__github__})')

    args = parser.parse_args()

    # Configure logging
    if args.json or args.short:
        # Quiet modes: only show errors
        log_level = logging.ERROR
        log_stream = sys.stderr
    elif args.no_warnings:
        # No warnings: show info but not warnings
        log_level = logging.INFO
        log_stream = sys.stdout
    else:
        # Normal mode: show info and warnings
        log_level = logging.INFO
        log_stream = sys.stdout

    logging.basicConfig(
        level=log_level,
        format='%(message)s',
        stream=log_stream,
        force=True
    )

    # Configure output formatting
    if args.no_color or args.json or args.short:
        for attr in dir(Colors):
            if not attr.startswith('_'):
                setattr(Colors, attr, '')

    # Handle comparison mode
    if args.compare:
        old_file, new_file = args.compare
        print(f"{Colors.BOLD}{Colors.HEADER}üîç ARM Stack Usage Analyzer - Comparison Mode{Colors.END}")
        print(f"{Colors.CYAN}Comparing: {old_file} vs {new_file}{Colors.END}")

        old_data = load_json_data(old_file)
        new_data = load_json_data(new_file)

        if old_data and new_data:
            compare_analyses(old_data, new_data)
        else:
            print(f"{Colors.RED}‚ùå Failed to load comparison files{Colors.END}")
            sys.exit(1)
        return

    # Handle JSON reading/filtering mode
    if args.read_json:
        if not args.json and not args.short:
            print(f"{Colors.BOLD}{Colors.HEADER}üîç ARM Stack Usage Analyzer - JSON Reader Mode{Colors.END}")

        output_format = 'json' if args.json else ('short' if args.short else 'table')
        # Use threshold from command line (0 means show all)
        threshold = args.threshold if args.threshold > 10 else 0  # Only filter if threshold was explicitly set
        read_and_display_json(args.read_json, threshold, output_format)
        return

    # Handle legacy JSON filtering mode (deprecated)
    if args.filter_json:
        if not args.json and not args.short:
            print(f"{Colors.BOLD}{Colors.HEADER}üîç ARM Stack Usage Analyzer - JSON Filter Mode (deprecated){Colors.END}")
            print(f"{Colors.YELLOW}‚ö†Ô∏è  --filter-json is deprecated, use --read-json instead{Colors.END}")

        output_format = 'json' if args.json else ('short' if args.short else 'table')
        read_and_display_json(args.filter_json, args.threshold, output_format)
        return

    # Require build directory for analysis mode
    if not args.build_dir:
        parser.error("build_dir is required unless using --compare, --read-json, or --filter-json")

    # Display tool header (unless in JSON or short mode)
    if not args.json and not args.short:
        print(f"{Colors.BOLD}{Colors.HEADER}üîç ARM Stack Usage Analyzer{Colors.END}")
        print(f"{Colors.CYAN}Build directory: {args.build_dir}{Colors.END}")
        if args.threshold != 10:
            print(f"{Colors.CYAN}Threshold: {args.threshold} bytes{Colors.END}")
        if args.no_warnings:
            print(f"{Colors.CYAN}Warnings: disabled{Colors.END}")
        if args.export_json:
            print(f"{Colors.CYAN}JSON export: {args.export_json}{Colors.END}")
        if args.objdump != DEFAULT_OBJDUMP:
            print(f"{Colors.CYAN}Objdump: {args.objdump}{Colors.END}")
        print()

    # Initialize and run analysis
    analyzer = StackAnalyzer(args.build_dir, args.threshold, not args.no_warnings, args.objdump)

    try:
        # Verification and setup phase
        obj_files = analyzer.check_prerequisites()

        # Analysis workflow
        analyzer.parse_object_files(obj_files)
        analyzer.resolve_symbols()
        analyzer.create_interrupt_node()
        analyzer.trace_call_graph()

        if args.json:
            # JSON mode: generate data and output to stdout
            analysis_data = analyzer.generate_analysis_data()
            print(json.dumps(analysis_data, indent=2, sort_keys=True))
        elif args.short:
            # Short mode: output only function list
            analyzer.generate_short_report()
        else:
            # Normal mode: generate report
            analysis_data = analyzer.generate_report()

            # Export to JSON if requested
            if args.export_json:
                analyzer.export_json(args.export_json, analysis_data)

    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}‚ö†Ô∏è  Analysis interrupted by user{Colors.END}")
        sys.exit(1)
    except Exception as e:
        print(f"{Colors.RED}‚ùå Error during analysis: {e}{Colors.END}")
        sys.exit(1)


if __name__ == "__main__":
    main()
